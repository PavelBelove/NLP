{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n",
    "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n",
    "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n",
    "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n",
    "5. сравнить все реализованные методы сделать выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4611,
     "status": "ok",
     "timestamp": 1617782512502,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yPRx8Cu_RDY1",
    "outputId": "9595e8e4-9731-4cfe-a67f-cfb0d9759cd4"
   },
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "#import gensim.downloader as api\n",
    "#word_vectors = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "vXxwW9NzW570"
   },
   "source": [
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2783,
     "status": "ok",
     "timestamp": 1617782637610,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "tpwgA3svWiRw",
    "outputId": "9d4e2aa6-bcc7-4793-f85d-2ce708f88ff8"
   },
   "source": [
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1617782704463,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])\n",
    "    \n",
    "    \n",
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sendsms.megafon.ru/status/send/0123456789ABCDEF', 'X')\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train:\n",
    "    for token in sent:\n",
    "        if len(token.form) == 47:\n",
    "            print((token.form, token.upos))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23568564014423887"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119991237825633"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [\n",
    "                      UnigramTagger, \n",
    "                      BigramTagger, \n",
    "                      TrigramTagger\n",
    "                     ],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "        \n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.9435261011694133\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9471657735988946\n",
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9487497051191319\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.7532184140464395\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.7722255922892866\n",
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.7532605398847437\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 1198385)\n",
      "TfidfVectorizer_char + HashingVectorizer_word : 0.9441327132409935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "X_train_1 = coder_1.fit_transform(train_tok)\n",
    "X_test_1 = coder_1.transform(test_tok)\n",
    "\n",
    "X_train_2 = coder_2.fit_transform(train_tok)\n",
    "X_test_2 = coder_2.transform(test_tok)\n",
    "\n",
    "\n",
    "X_train = hstack((X_train_1,X_train_2))\n",
    "X_test = hstack((X_test_1,X_test_2))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)    \n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c599ce61703c4696900b7c90b508119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['но', 'кто-то', 'идти', 'я', 'навстречу', 'и', ',', 'мочь', 'быть', ',', 'пройти', 'уже', 'пол', 'путь', '…']\n",
      "['CCONJ', 'PRON', 'VERB', 'PRON', 'ADV', 'CCONJ', 'PUNCT', 'VERB', 'VERB', 'PUNCT', 'VERB', 'ADV', 'NUM', 'NOUN', 'PUNCT']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950de95073b4934ac415e570c9a511a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['поэтому', 'ничего', 'не', 'измениться', '.']\n",
      "['ADV', 'PRON', 'PART', 'VERB', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "just_texts_trian=[]\n",
    "just_poss_train =[]\n",
    "for sent in tqdm_notebook(fdata_train):\n",
    "    just_text_sent = []\n",
    "    just_pos_sent = []\n",
    "    for point in sent:\n",
    "        just_text_sent.append(morpher.parse(point[0].lower())[0].normal_form)\n",
    "        just_pos_sent.append(point[1])\n",
    "    \n",
    "    just_texts_trian.append(just_text_sent)\n",
    "    just_poss_train.append(just_pos_sent)\n",
    "        \n",
    "    \n",
    "print(just_texts_trian[-1])\n",
    "print(just_poss_train[-1])\n",
    "\n",
    "\n",
    "just_texts_test=[]\n",
    "just_poss_test =[]\n",
    "for sent in tqdm_notebook(fdata_test):\n",
    "    just_text_sent = []\n",
    "    just_pos_sent = []\n",
    "    for point in sent:\n",
    "        just_text_sent.append(morpher.parse(point[0].lower())[0].normal_form)        \n",
    "        just_pos_sent.append(point[1])\n",
    "    \n",
    "    just_texts_test.append(just_text_sent)\n",
    "    just_poss_test.append(just_pos_sent)\n",
    "        \n",
    "    \n",
    "print(just_texts_test[-1])\n",
    "print(just_poss_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = just_texts_trian\n",
    "\n",
    "dimention = 200   # int(39694**(1/2)) = 200\n",
    "\n",
    "modelW2V = Word2Vec(sentences=sentences, vector_size=dimention, window=5, min_count=1)\n",
    "modelFT = FastText(sentences=sentences, window=5, min_count=1, vector_size=dimention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized_data(model, num=1):\n",
    "    train_tok = []\n",
    "    train_words = []\n",
    "    train_label = []\n",
    "    \n",
    "    for i in range(len(just_texts_trian)):\n",
    "        for j in range(len(just_texts_trian[i])):\n",
    "            nums_list=[]\n",
    "            truncated_sent =''\n",
    "            try:\n",
    "            \n",
    "                for k in range(num):\n",
    "                    if (j >= k):\n",
    "                        result = model.wv[just_texts_trian[i][j-k]]\n",
    "                        word_result = just_texts_trian[i][j-k]\n",
    "                    else:\n",
    "                        result = np.zeros(dimention)\n",
    "                        word_result = '-'\n",
    "                        \n",
    "                    nums_list = nums_list + list(result)\n",
    "                    truncated_sent = truncated_sent + word_result + ' '\n",
    "                    \n",
    "                train_tok.append(np.array(nums_list))\n",
    "                train_words.append(truncated_sent)    \n",
    "                train_label.append(just_poss_train[i][j])\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "    test_tok = []\n",
    "    test_words = []    \n",
    "    test_label = []            \n",
    "    for i in range(len(just_texts_test)):\n",
    "        for j in range(len(just_texts_test[i])):\n",
    "            nums_list=[]\n",
    "            truncated_sent =''\n",
    "            try:\n",
    "            \n",
    "                for k in range(num):\n",
    "                    if (j >= k):\n",
    "                        result = model.wv[just_texts_test[i][j-k]]\n",
    "                        word_result = just_texts_test[i][j-k]\n",
    "                    else:\n",
    "                        result = np.zeros(dimention)\n",
    "                        word_result = '-'\n",
    "                        \n",
    "                    nums_list = nums_list + list(result)\n",
    "                    truncated_sent = truncated_sent + word_result + ' '\n",
    "                    \n",
    "                test_tok.append(np.array(nums_list))\n",
    "                test_words.append(truncated_sent)                        \n",
    "                test_label.append(just_poss_test[i][j])\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "    return train_tok, train_label, test_tok, test_label, train_words, test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(model, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_label), len(train_tok), len(test_label), len(test_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, num=1):\n",
    "    train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(model, num=num)\n",
    "    print(len(train_label), len(train_tok), len(test_label), len(test_tok))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train_enc_labels = le.fit_transform(train_label) \n",
    "    test_enc_labels = le.transform(test_label)\n",
    "    print(le.classes_)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_tok = scaler.fit_transform(train_tok)\n",
    "    test_tok = scaler.fit_transform(test_tok)    \n",
    "\n",
    "\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, verbose=1, n_jobs=-1)\n",
    "    lr.fit(train_tok, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(test_tok)\n",
    "\n",
    "    print('LogisticRegression', 'word vector bild on', num, 'item(s)')\n",
    "    print(model)\n",
    "    print('Scor: ', accuracy_score(test_enc_labels, pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Протестируем FastText с разными эмбедингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 1 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.901914198092542\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 2 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9022512047989755\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 3 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9012233343443534\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 4 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9018046709129511\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 13.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 5 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9018720722542378\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Протестируем Word2Vec с разными эмбедингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 114586 114586\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 1 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8177875133087812\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 110739 110739\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 2 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8294096930620648\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 107686 107686\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 3 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8325594784837398\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 105044 105044\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 4 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8317181371615704\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 102719 102719\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 5 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8303235039281924\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
       "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X',\n",
       "       None], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 39694\n",
    "sequence_length = 3\n",
    "\n",
    "train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(modelFT, num=sequence_length)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((1024,), (1024,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((1024,), (1024,)), types: (tf.string, tf.int32)>\n",
      "Epoch 1/15\n",
      "851/851 [==============================] - 88s 100ms/step - loss: 0.8171 - accuracy: 0.7279 - val_loss: 0.4792 - val_accuracy: 0.8103\n",
      "Epoch 2/15\n",
      "851/851 [==============================] - 85s 99ms/step - loss: 0.3528 - accuracy: 0.8521 - val_loss: 0.4485 - val_accuracy: 0.8213\n",
      "Epoch 3/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.3111 - accuracy: 0.8672 - val_loss: 0.4512 - val_accuracy: 0.8231\n",
      "Epoch 4/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2908 - accuracy: 0.8752 - val_loss: 0.4564 - val_accuracy: 0.8243\n",
      "Epoch 5/15\n",
      "851/851 [==============================] - 85s 99ms/step - loss: 0.2750 - accuracy: 0.8815 - val_loss: 0.4622 - val_accuracy: 0.8253\n",
      "Epoch 6/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2620 - accuracy: 0.8866 - val_loss: 0.4709 - val_accuracy: 0.8260\n",
      "Epoch 7/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2510 - accuracy: 0.8909 - val_loss: 0.4791 - val_accuracy: 0.8257\n",
      "Epoch 8/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2420 - accuracy: 0.8941 - val_loss: 0.4888 - val_accuracy: 0.8244\n",
      "Epoch 9/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2346 - accuracy: 0.8967 - val_loss: 0.4953 - val_accuracy: 0.8247\n",
      "Epoch 10/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2281 - accuracy: 0.8990 - val_loss: 0.5029 - val_accuracy: 0.8244\n",
      "Epoch 11/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2225 - accuracy: 0.9014 - val_loss: 0.5108 - val_accuracy: 0.8242\n",
      "Epoch 12/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2175 - accuracy: 0.9030 - val_loss: 0.5172 - val_accuracy: 0.8239\n",
      "Epoch 13/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2130 - accuracy: 0.9048 - val_loss: 0.5270 - val_accuracy: 0.8236\n",
      "Epoch 14/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.2088 - accuracy: 0.9065 - val_loss: 0.5341 - val_accuracy: 0.8227\n",
      "Epoch 15/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.2051 - accuracy: 0.9079 - val_loss: 0.5401 - val_accuracy: 0.8230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d423b03130>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_words, train_enc_labels))\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((test_words, test_enc_labels))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "text_ds = train_dataset.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "embedding_dim=200    # int(39694**(1/2)) = 200\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  Bidirectional(LSTM(units=32, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(len(set(train_label)), activation=tf.keras.activations.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_dataset,\n",
    "    validation_data=test_dataset, \n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для __nltk.tag__ лучший вариант это:\n",
    "Комбинация из  __DefaultTagger UnigramTagger BigramTagger TrigramTagger__  \n",
    "0.9119991237825633\n",
    "\n",
    "Для __Vectorizer__ лучший вариант это:\n",
    "Комбинация из __LogisticRegression__ поверх __TfidfVectorizer__ при условии __analyzer='char'__  \n",
    "0.9487497051191319\n",
    "\n",
    "Для __FastText__, __Word2Vec__ лучший вариант это:\n",
    "Комбинация из __LogisticRegression__ поверх __FastText__ при условии эмбединга из __2-х слов__  \n",
    "0.9022512047989755\n",
    "\n",
    "Для __LSTM NN__ c __Embedding__ лучший вариант это:  \n",
    "0.8260\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данной задачи побуквенный подход к кодированию слов выглядит предпочтительней чем по словный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2. Проверить насколько хорошо работает NER\n",
    "\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "\n",
    "1) взять нер из nltk\n",
    "\n",
    "2) проверить deeppavlov\n",
    "\n",
    "3) написать свой нер попробовать разные подходы\n",
    "\n",
    "    a) передаём в сетку токен и его соседей\n",
    "    b)передаём в сетку только токен\n",
    "    \n",
    "4) сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 13:49:45--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
      "Распознаётся ai-center.botik.ru (ai-center.botik.ru)… 95.129.138.2\n",
      "Подключение к ai-center.botik.ru (ai-center.botik.ru)|95.129.138.2|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 3363777 (3,2M) [application/zip]\n",
      "Сохранение в: «./Persons-1000.zip»\n",
      "\n",
      "./Persons-1000.zip  100%[===================>]   3,21M  1,23MB/s    за 2,6s    \n",
      "\n",
      "2021-07-21 13:49:48 (1,23 MB/s) - «./Persons-1000.zip» сохранён [3363777/3363777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O './Persons-1000.zip' 'http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "\n",
    "path = 'Persons-1000.zip'\n",
    "records = corus.persons.load_persons(path)\n",
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PersonsSpan(\n",
       "     id=1,\n",
       "     start=362,\n",
       "     stop=398,\n",
       "     value='ТОМАС ХАММАРБЕРГ (THOMAS HAMMARBERG)'\n",
       " ),\n",
       " PersonsSpan(\n",
       "     id=2,\n",
       "     start=624,\n",
       "     stop=634,\n",
       "     value='ХАММАРБЕРГ'\n",
       " )]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. взять нер из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/pavel/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package punkt to /home/pavel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pavel/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package words to /home/pavel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Thomas Hammarberg', 'PERSON'),\n",
       " ('Комиссар', 'PERSON'),\n",
       " ('МОСКВА', 'ORGANIZATION'),\n",
       " ('РИА Новости', 'ORGANIZATION'),\n",
       " ('СЕ', 'ORGANIZATION'),\n",
       " ('Совета Европы', 'PERSON'),\n",
       " ('Хаммарберг', 'PERSON')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in \n",
    " nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. проверить deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "  Using cached deeppavlov-0.15.0-py3-none-any.whl (907 kB)\n",
      "Collecting pymorphy2==0.8\n",
      "  Using cached pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: pyopenssl==19.1.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (19.1.0)\n",
      "Requirement already satisfied: uvicorn==0.11.7 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (0.11.7)\n",
      "Requirement already satisfied: numpy==1.18.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (1.18.0)\n",
      "Collecting fastapi==0.47.1\n",
      "  Using cached fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
      "Requirement already satisfied: requests==2.22.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (2.22.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Using cached Cython-0.29.14-cp38-cp38-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting pytelegrambotapi==3.6.7\n",
      "  Using cached pyTelegramBotAPI-3.6.7-py3-none-any.whl\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: uvloop==0.14.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: click==7.1.2 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: pydantic==1.3 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (1.3)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: tqdm==4.41.1 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (4.41.1)\n",
      "Collecting prometheus-client==0.7.1\n",
      "  Using cached prometheus_client-0.7.1-py3-none-any.whl\n",
      "Collecting pandas==0.25.3\n",
      "  Using cached pandas-0.25.3-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting nltk==3.4.5\n",
      "  Using cached nltk-3.4.5-py3-none-any.whl\n",
      "Requirement already satisfied: pytz==2019.1 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (3.0.12)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (1.4.1)\n",
      "Collecting overrides==2.7.0\n",
      "  Using cached overrides-2.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: h5py==2.10.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /home/pavel/anaconda3/lib/python3.8/site-packages (from deeppavlov) (2.4.417127.4579844)\n",
      "Collecting ruamel.yaml==0.15.100\n",
      "  Using cached ruamel.yaml-0.15.100-cp38-cp38-linux_x86_64.whl\n",
      "Collecting aio-pika==6.4.1\n",
      "  Using cached aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n",
      "Requirement already satisfied: yarl in /home/pavel/anaconda3/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (1.6.3)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /home/pavel/anaconda3/lib/python3.8/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: six in /home/pavel/anaconda3/lib/python3.8/site-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/pavel/anaconda3/lib/python3.8/site-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/pavel/anaconda3/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /home/pavel/anaconda3/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /home/pavel/anaconda3/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: cryptography>=2.8 in /home/pavel/anaconda3/lib/python3.8/site-packages (from pyopenssl==19.1.0->deeppavlov) (2.9.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/pavel/anaconda3/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pavel/anaconda3/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/pavel/anaconda3/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/pavel/anaconda3/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (1.25.9)\n",
      "Requirement already satisfied: joblib in /home/pavel/anaconda3/lib/python3.8/site-packages (from sacremoses==0.0.35->deeppavlov) (0.16.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/pavel/anaconda3/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: websockets==8.* in /home/pavel/anaconda3/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
      "Requirement already satisfied: httptools==0.1.* in /home/pavel/anaconda3/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /home/pavel/anaconda3/lib/python3.8/site-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /home/pavel/anaconda3/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
      "Requirement already satisfied: multidict>=4.0 in /home/pavel/anaconda3/lib/python3.8/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (5.1.0)\n",
      "Installing collected packages: ruamel.yaml, pytelegrambotapi, pymorphy2, prometheus-client, pandas, overrides, nltk, fastapi, Cython, aio-pika, deeppavlov\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel-yaml 0.15.87\n",
      "\u001b[31mERROR: Cannot uninstall 'ruamel-yaml'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python -m venv env\n",
    "# !pip install deeppavlov\n",
    "# !python -m deeppavlov install squad_bert\n",
    "# !python -m deeppavlov install ner_ontonotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deeppavlov'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a12dc126f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deeppavlov'"
     ]
    }
   ],
   "source": [
    "# import deeppavlov\n",
    "# from deeppavlov import configs, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeppavlov_ner(word_tag[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeppavlov, увы, не встал, разбираться нет времени по дедлайну."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /home/pavel/anaconda3/lib/python3.8/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from razdel import tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PersonsSpan(\n",
       "     id=1,\n",
       "     start=308,\n",
       "     stop=324,\n",
       "     value='ГРИГОРИЙ КАРАСИН'\n",
       " ),\n",
       " PersonsSpan(\n",
       "     id=2,\n",
       "     start=387,\n",
       "     stop=402,\n",
       "     value='ДЭНИЭЛ ФРИД'\n",
       " )]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import corus\n",
    "\n",
    "path = 'Persons-1000.zip'\n",
    "records = corus.persons.load_persons(path)\n",
    "rec = next(records)\n",
    "rec.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "       \n",
    "        result = 'None'        \n",
    "        \n",
    "        for item in rec.spans:\n",
    "            \n",
    "            if (token.start >= item.start) and (token.stop <= item.stop): \n",
    "                result = 'PER'\n",
    "                break\n",
    "        \n",
    "                \n",
    "    \n",
    "        words.append([token.text, result])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None    244180\n",
       "PER      21167\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
    "\n",
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "        return input_data\n",
    "\n",
    "def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n",
    "    \n",
    "    vocab_size = 30000\n",
    "    #seq_len = 1\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "    text_data = train_data.map(lambda x, y: x)\n",
    "    vectorize_layer.adapt(text_data)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(len(df_words['tag'].value_counts()), activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        return self.fc3(concat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12439/12439 [==============================] - 254s 20ms/step - loss: 0.1408 - accuracy: 0.9535 - val_loss: 0.0711 - val_accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "12439/12439 [==============================] - 216s 17ms/step - loss: 0.0346 - accuracy: 0.9886 - val_loss: 0.0736 - val_accuracy: 0.9762\n",
      "Epoch 3/5\n",
      "12439/12439 [==============================] - 229s 18ms/step - loss: 0.0317 - accuracy: 0.9892 - val_loss: 0.0860 - val_accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "12439/12439 [==============================] - 230s 19ms/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "12439/12439 [==============================] - 175s 14ms/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.0784 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda9856ef70>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "model = modelNER()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-churned       0.98      1.00      0.99     61011\n",
      "     Churned       0.98      0.72      0.83      5326\n",
      "\n",
      "    accuracy                           0.98     66337\n",
      "   macro avg       0.98      0.86      0.91     66337\n",
      "weighted avg       0.98      0.98      0.97     66337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "import numpy as np\n",
    "\n",
    "report = classification_report(valid_y, np.around(mmodel.predict(valid_x)[:, 1]).astype('int'), target_names=['Non-churned', 'Churned'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYWUe+YbrGuKh6AT1qUB4a",
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
