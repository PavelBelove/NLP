{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31704,
     "status": "ok",
     "timestamp": 1626350967438,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "OlqOAQmQGXOL",
    "outputId": "aca2c4aa-6c6d-4cd8-fecf-17fc9dbaecaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  imdb.zip\r\n",
      "replace test.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!wget -O imdb.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vrQ5czMHoO3pEnmofFskymXMkq_u1dPc\"\n",
    "!unzip imdb.zip\n",
    "!pip -q install eli5\n",
    "!pip -q install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF80-qhXt17P"
   },
   "source": [
    "Тут придется авторизировать запрос к гугл драйву, извините."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRdS0CM2rSzV"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1d1-5FwxK53ePwygNWeG7jhsOWZbi5HOv'})\n",
    "downloaded.GetContentFile('train_docs.pkl')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1MMOY477t965G0C5DtXeREVp0X85UaNq5'})\n",
    "downloaded.GetContentFile('test_docs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbkG4qoS0AB"
   },
   "source": [
    "# Классификация текстов\n",
    "\n",
    "Начнём с самого простого - анализа тональности текста.\n",
    "\n",
    "Будем классифицировать отзывы с IMDB на положительные/отрицательные.\n",
    "\n",
    "Датасет взят с http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1626351026451,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Js5kqmWMGqN4",
    "outputId": "a9f541aa-2760-4802-c5c4-1356cebcc96f"
   },
   "outputs": [],
   "source": [
    "!head train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1395,
     "status": "ok",
     "timestamp": 1626351031134,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "5R9e3pctHIV2",
    "outputId": "34384220-8813-47b8-f4fb-1abc96ac5462"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print('Train size = {}'.format(len(train_df)))\n",
    "print('Test size = {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHbJCPPwhjp"
   },
   "source": [
    "Посмотрите глазами на тексты? Какие есть зацепки, как определить, что это за сентимент?\n",
    "\n",
    "Самое простое, как всегда - найти ключевые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1617972489868,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "DywUCyMLr_TD",
    "outputId": "89faa95b-d06b-445c-ae31-3529c5602cb9"
   },
   "outputs": [],
   "source": [
    "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
    "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
    "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
    "\n",
    "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
    "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
    "is_positive = positives_count > negatives_count\n",
    "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
    "\n",
    "accuracy = correct_count / len(test_df)\n",
    "\n",
    "print('Test accuracy = {:.2%}'.format(accuracy))\n",
    "if accuracy > 0.71:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo8HRABxv1kW"
   },
   "source": [
    "**Задание** Придумайте хорошие ключевые слова или фразы и наберите хотя бы 71% точности на тесте (и не забудьте посмотреть на код классификации!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaIrBClMUHZB"
   },
   "source": [
    "**Задание** Кому-нибудь нравятся эти `<br /><br />`? Лично мне - нет. Напишите регулярку, которая будет их удалять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1626351034439,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "09OkUmtde6ny",
    "outputId": "d206e083-12c9-4184-9862-b20220d796dc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('<br />')\n",
    "\n",
    "print(train_df['review'].iloc[3])\n",
    "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO6D9NuMi4II"
   },
   "source": [
    "Применим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LTwQqs_hD-K"
   },
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
    "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGxzf4oXmXqw"
   },
   "source": [
    "Пора переходить к машинке!\n",
    "\n",
    "Как будем представлять текст? Проще всего - мешком слов.\n",
    "\n",
    "Заведём большой-большой словарь - список всех слов в обучающей выборке. Тогда каждое предложение можно представить в виде вектора, в котором будет записано, сколько раз встретилось каждое из возможных слов:\n",
    "\n",
    "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
    "\n",
    "Простой и приятный способ сделать это - запихнуть тексты в `CountVectorizer`.\n",
    "\n",
    "Он имеет такую сигнатуру:\n",
    "\n",
    "```python\n",
    "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
    "```\n",
    "\n",
    "Для начала обратим внимание на параметры `lowercase=True` и `max_df=1.0, min_df=1, max_features=None` - они про то, что по умолчанию все слова будут приводиться к нижнему регистру и в словарь попадут все слова, встречавшиеся в текстах.\n",
    "\n",
    "При желании можно было бы убрать слишком редкие или слишком частотные слова - пока не будем этого делать.\n",
    "\n",
    "Посмотрим на простом примере, как он будет работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1626351039953,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "5Odnum4iyGDr",
    "outputId": "bf5c38b3-2d69-4ca4-d4e5-3e9edd274ad8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "\n",
    "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
    "\n",
    "print(dummy_matrix.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3zC1ItWybVc"
   },
   "source": [
    "*Как именно vectorizer определяет границы слов? Обратите внимание на параметр `token_pattern=r'(?u)\\b\\w\\w+\\b'` - как он будет работать?*\n",
    "\n",
    "Запустим его на реальных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15100,
     "status": "ok",
     "timestamp": 1626351056910,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Ccd2gaCdQq2W",
    "outputId": "19ff3158-295d-4cf7-ce83-b0b2d38b2b89"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_JC9n6C0bFR"
   },
   "source": [
    "Посмотрим на слова, попавшие в словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1626351057506,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "stV4ICO3mKsf",
    "outputId": "f5d6493d-17c5-46fd-f372-9142f2003a03"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oesbuQ9g0krj"
   },
   "source": [
    "Попробуем кого-нибудь таки сконвертировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1626351057507,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "lUWtDWcp0g7U",
    "outputId": "5879187f-dfd0-4fda-abf2-523a7960dfe5"
   },
   "outputs": [],
   "source": [
    "vectorizer.transform([train_df['review'].iloc[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZCRef3pyCRC"
   },
   "source": [
    "То, что и хотели - вектор с bow (т.е. bag-of-words) представлением исходного текста.\n",
    "\n",
    "И чем эта информация может помочь? Ну, всё тем же - какие-то слова носят положительный окрас, какие-то - отрицательный. Большинство вообще нейтральный, да.\n",
    "\n",
    "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
    "\n",
    "Хочется, наверное, подобрать коэффициенты, которые будут определять уровень окраса, да? Подбирать нужно по обучающей выборке, а не как мы перед этим делали.\n",
    "\n",
    "Например, для выборки\n",
    "```\n",
    "1   The movie was excellent\n",
    "0   the movie was awful\n",
    "```\n",
    "легко подобрать коэффициенты на глазок: что-нибудь вроде `+1` для `excellent`,  `-1` для `awful` и по нулям всем остальным.\n",
    "\n",
    "Построим линейную модель, которая станет этим заниматься. Она будет учиться строить разделяющую гиперплоскость в пространстве bow-векторов.\n",
    "\n",
    "Проверим, как справится логистическая регрессия с нашей супер-выборкой из пары предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1626351057508,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "i6WVgK4LtUn2",
    "outputId": "1ba80652-2e2b-45bf-e74f-f4ad7f5d1f03"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "dummy_labels = [1, 0]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(dummy_data, dummy_labels)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9Y-kq-tv-XY"
   },
   "source": [
    "Получилось что надо.\n",
    "\n",
    "Запустим теперь её на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15927,
     "status": "ok",
     "timestamp": 1626351073430,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "kvxHIIbSiUXq",
    "outputId": "0af1a15b-a824-4469-80dc-e9e1f7480393"
   },
   "outputs": [],
   "source": [
    "model.fit(train_df['review'], train_df['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4654,
     "status": "ok",
     "timestamp": 1626351078066,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "-d3BBV_uUu-O",
    "outputId": "2147913d-0268-44a3-d40d-d55feef70249"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, test_df):\n",
    "    preds = model.predict(test_df['review'])\n",
    "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
    "    \n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnmmz3u71ctO"
   },
   "source": [
    "Прогресс!\n",
    "\n",
    "Хочется как-то посмотреть, что заинтересовало классификатор. К счастью, сделать это совсем просто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1626351080461,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "8W1Ngl-aVuYx",
    "outputId": "c92f9c75-2b88-48df-e9ae-1eb4687e135a"
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(classifier, vec=vectorizer, top=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VgCE9tDk-aO"
   },
   "source": [
    "Посмотрим на конкретные примеры его работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1626351080463,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "v-4sVWBOWJpo",
    "outputId": "f3410bea-8dbb-44fb-a72d-04e5d2d378f7"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1626351080829,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "AoZhtlYlW-xG",
    "outputId": "15eba8b3-55d4-45a3-f8b9-68ddfb819858"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[6] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[6], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNNUqZvplhAC"
   },
   "source": [
    "Посмотрим на примеры неправильной классификации, наконец:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 5219,
     "status": "ok",
     "timestamp": 1626351086040,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yf9ZzS8fXKFm",
    "outputId": "5b82d3c3-fd59-48ac-de7b-42ec528f80d2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = model.predict(test_df['review'])\n",
    "incorrect_pred_index = np.random.choice(np.where(preds != test_df['is_positive'])[0])\n",
    "\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[incorrect_pred_index],\n",
    "                     vec=vectorizer, targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbFXKNrngP46"
   },
   "source": [
    "## Придумываем новые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz7GSFzIlv4V"
   },
   "source": [
    "### Tf-idf\n",
    "\n",
    "Сейчас мы на все слова смотрим с одинаковым весом - хотя какие-то из них более редкие, какие-то более частые, и эта частотность - полезная, вообще говоря, информация.\n",
    "\n",
    "Самый простой способ добавить статистическую информацию о частотностях - сделать *tf-idf* взвешивание:\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
    "\n",
    "*tf* - term-frequency - частотность слова `t` в конкретном документе `d` (рецензии в нашем случае). Это ровно то, что мы уже считали.\n",
    "\n",
    "*idf* - inverse document-frequency - коэффициент, который тем больше, чем в меньшем числе документов встречалось данное слово. Считается как-нибудь так:\n",
    "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
    "где $n_d$ - число всех документов, а $n_{d(t)}$ - число документов со словом `t`.\n",
    "\n",
    "Использовать его просто - нужно заменить `CountVectorizer` на `TfidfVectorizer`.\n",
    "\n",
    "**Задание** Попробуйте запустить `TfidfVectorizer`. Посмотрите на ошибки, которые он научился исправлять, и на ошибки, которые он начал делать - по сравнению с `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22675,
     "status": "ok",
     "timestamp": 1626351108712,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "p3DjjiJglvT3",
    "outputId": "39599c51-f5d5-4f8e-9a6e-49ca8a2528bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe_CJxQ0tFP9"
   },
   "source": [
    "### N-граммы слов\n",
    "\n",
    "До сих пор мы смотрели на тексты как на мешок слов - но очевидно, что есть разница между `good movie` и `not good movie`.\n",
    "\n",
    "Добавим информацию (хоть какую-то) о последовательностях слов - будем извлекать еще и биграммы слов.\n",
    "\n",
    "В Vectorizer'ах для этого есть параметр `ngram_range=(n_1, n_2)` - он говорит, что нужны n_1-...n_2-граммы.\n",
    "\n",
    "**Задание** Попробуйте увеличенный range и поинтерпретируйте полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53057,
     "status": "ok",
     "timestamp": 1626351161749,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "RDpdrT0HuKYN",
    "outputId": "8cd74bf3-e6f0-4e9f-8a7f-bb48f801bfbf"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrBoThj6wl2F"
   },
   "source": [
    "### N-граммы символов\n",
    "\n",
    "Символьные n-граммы дают простой способ выучить полезные корни и суффиксы, не связываясь с этой вашей лингвистикой - только статистика, только хардкор.\n",
    "\n",
    "Например, слово `badass` мы можем представить в виде такой последовательности триграмм:\n",
    "\n",
    "`##b #ba bad ada das ass ss# s##`\n",
    "\n",
    "So interpretable, неправда ли?\n",
    "\n",
    "Реализовать это дело всё так же просто - нужно поставить `analyzer='char'` в вашем любимом Vectorizer'е и выбрать размер `ngram_range`.\n",
    "\n",
    "**Задание** Запилите классификатор на n-граммах символов и визуализируйте его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304569,
     "status": "ok",
     "timestamp": 1626351466314,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "QFaWmUrGyY3n",
    "outputId": "2ace6672-b3cc-4701-bbaa-db63a67756b3"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1626351466965,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "9E1E1Bn8yvhq",
    "outputId": "4705f921-3c56-411c-96c8-2e7a10487558"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fZ6I8mN0VPU"
   },
   "source": [
    "## Подключаем лингвистику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkywIvbp4N-L"
   },
   "source": [
    "### Лемматизация и стемминг\n",
    "\n",
    "Если присмотреться, можно найти формы одного слова с разной семантической окраской по мнению классификатора. Или нет?\n",
    "\n",
    "**Задание** Найти формы слова с разной семантической окраской.\n",
    "\n",
    "Поверя, что они есть, попробуем что-нибудь с этим сделать.\n",
    "\n",
    "Например, лемматизируем - сведем к начальной форме все слова. Поможет в этом библиотека spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5CTTdtxI5yv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "\n",
    "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1626353224187,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "bXs8Ia_bqeS0",
    "outputId": "2f9db94c-7417-4e25-f85f-387c332966e9"
   },
   "outputs": [],
   "source": [
    "for token in docs[0]:\n",
    "    print(token.text, token.lemma_, token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVXJr_xxqlPK"
   },
   "source": [
    "Весь этот процесс очень долгий, поэтому я предподсчитал всё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZNz7E5JqrWz"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_docs.pkl', 'rb') as f:\n",
    "    train_docs = pickle.load(f)\n",
    "    \n",
    "with open('test_docs.pkl', 'rb') as f:\n",
    "    test_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pfup3O5r30m"
   },
   "source": [
    "**Задание** Сделайте классификатор на лемматизированных текстах.\n",
    "\n",
    "Более простой способ нормализации слов - использовать стемминг. Он немного тупой, не учитывает контекст, но иногда оказывается даже эффективнее лемматизации - а, главное, быстрее.\n",
    "\n",
    "По сути это просто набор правил, как обрезать слово, чтобы получить основу (stem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310498,
     "status": "ok",
     "timestamp": 1617972825511,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Cr0w_hVyrqFx",
    "outputId": "633f5dd1-5656-457c-c120-40acdc8ecdfa"
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('become'))\n",
    "print(stemmer.stem('becomes'))\n",
    "print(stemmer.stem('became'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxGSEqHNsfHy"
   },
   "source": [
    "**Задание** Попробуйте вместо лемм классифицировать основы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1NBpkwuspBX"
   },
   "source": [
    "### NER\n",
    "\n",
    "В текстах рецензий очень много именованных сущностей. Вот, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "executionInfo": {
     "elapsed": 308509,
     "status": "ok",
     "timestamp": 1617972825515,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Xki8Maf9MrVY",
    "outputId": "f0988ff1-b192-444b-d578-f8596e529b45"
   },
   "outputs": [],
   "source": [
    "displacy.render(docs[0], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGhcnuZSteHc"
   },
   "source": [
    "Вообще говоря, почему вдруг какой-нибудь Депп должен нести семантическую окраску? Однако оказывается, что классификатор выучивает, что какие-то имена чаще в положительных рецензиях - или наоборот. Это похоже на переобучение - почему бы не попробовать вырезать сущности?\n",
    "\n",
    "**Задание** Удалите из текстов какие-то из сущностей, пользуясь координатами из запикленных файлов. Описание сущностей можно посмотреть [здесь](https://spacy.io/api/annotation#named-entities). Запустите классификатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2fHA70b0zEZ"
   },
   "source": [
    "## Включаем дип лёрнинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1VCrM671XO5"
   },
   "source": [
    "Мы тут пришли deep learning'ом заниматься, а делаем почему-то модель на логистической регрессии. Как так?\n",
    "\n",
    "Попробуем запустить относительно стандартную модель для классификации текстов - сверточная сеть поверх словных эмбеддингов.\n",
    "\n",
    "Разбираться, что это за зверь, будем на следующих занятиях, а пока будем просто им пользоваться :)\n",
    "\n",
    "Каждое предложение нужно представлять набором слов - и сразу же начинаются проблемы. Во-первых, как ограничить длину предложения?\n",
    "\n",
    "Прикинем по гистограмме, какая длина нам подходит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1617972850890,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "02Kv8YUPbhzG",
    "outputId": "457dbb1c-caff-4dcc-8735-3cc78098efcf"
   },
   "outputs": [],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 2272,
     "status": "ok",
     "timestamp": 1617972857946,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "O477ZV1t1WIO",
    "outputId": "fb1053f0-abc6-4b91-cd6d-9b6849da2e3b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text.split())), bins='auto')\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXO4xi0u5m8l"
   },
   "source": [
    "Кроме этого, нужно перенумеровать как-то слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1617973036848,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "mIMGE7L-55fs",
    "outputId": "c399154c-82ae-4ff2-9c69-1a9a7c5a81c4"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
    "\n",
    "word2idx = {\n",
    "    '': 0,\n",
    "    '<unk>': 1\n",
    "}\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < 10:\n",
    "        break\n",
    "        \n",
    "    word2idx[word] = len(word2idx)\n",
    "    \n",
    "print('Words count', len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTptlmd1yD3J"
   },
   "source": [
    "**Задание** Сконвертируйте данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPP0cYdJ5VkE"
   },
   "outputs": [],
   "source": [
    "def convert(texts, word2idx, max_text_len):\n",
    "    data = np.zeros((len(texts), max_text_len), dtype=np.int)\n",
    "    \n",
    "    for inx, text in enumerate(texts):\n",
    "        result = []\n",
    "        for word in text.split():\n",
    "            if word in word2idx:\n",
    "                result.append(word2idx[word])\n",
    "        padding = [0]*(max_text_len - len(result))\n",
    "        data[inx] = np.array(padding + result[-max_text_len:], dtype=np.int)\n",
    "    return data\n",
    "\n",
    "X_train = convert(train_df.review, word2idx, 1000)\n",
    "X_test = convert(test_df.review, word2idx, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYb-p5ioyLUZ"
   },
   "source": [
    "Поставим учиться модельку на keras.\n",
    "\n",
    "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
    "1. Определить модель, например:\n",
    "```python \n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
    "```\n",
    "2. Задать функцию потерь и оптимизатор:\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "3. Запустить обучение:\n",
    "```python\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, y_test))\n",
    "```\n",
    "\n",
    "В NLP чаще всего ставятся задачи классификации, поэтому нужно запомнить такие функции потерь:\n",
    "\n",
    "*   **categorical_crossentropy** - для многоклассовой классификации, в качестве меток должны передаваться one-hot-encoding вектора\n",
    "*   **sparse_categorical_crossentropy** - аналогично предыдущему, но в качестве меток нужно передавать просто индексы соответствующих классов\n",
    "*   **binary_crossentropy** - для бинарной классификации\n",
    "\n",
    "\n",
    "В качестве оптимизатора обычно используют `sgd` или `adam`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1617976380962,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Ncrq09zRxEvN",
    "outputId": "453b501d-c0bf-414c-9e9a-667eddcc38e8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KddjCCo-w50h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1617977526745,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "dDjri3167vFf",
    "outputId": "a82ade6e-49f1-496f-e80a-ed1b590cfc69"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(units=10, activation='relu'),\n",
    "    Dense(units=10, activation='relu'),\n",
    "    \n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127725,
     "status": "ok",
     "timestamp": 1617977655745,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "lw93gTmq8gZl",
    "outputId": "e10a2d0b-0287-4295-e796-41943209d305"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=10, \n",
    "          validation_data=(X_test, test_df.is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBGdVRQTyynD"
   },
   "source": [
    "**Задание** Подсчитайте качество модели на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_text_classification_part1.ipynb",
   "provenance": [
    {
     "file_id": "12nrEX3JXTxsHWC-HpuwkTWyJybjmkZu-",
     "timestamp": 1617977674484
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
